{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpipe-art==0.5.0\n",
      "  Downloading openpipe_art-0.5.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting tenacity\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting vllm\n",
      "  Downloading vllm-0.11.0-cp38-abi3-manylinux1_x86_64.whl.metadata (17 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: requests in /opt/miniforge3/lib/python3.12/site-packages (2.32.4)\n",
      "Collecting lxml\n",
      "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/miniforge3/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: transformers in /opt/miniforge3/lib/python3.12/site-packages (4.57.0)\n",
      "Requirement already satisfied: torch in /opt/miniforge3/lib/python3.12/site-packages (2.8.0)\n",
      "Collecting gql==3.4.1\n",
      "  Downloading gql-3.4.1-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: peft in /opt/miniforge3/lib/python3.12/site-packages (0.17.1)\n",
      "Collecting litellm==1.74.1 (from openpipe-art==0.5.0)\n",
      "  Downloading litellm-1.74.1-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting openai<=1.99.1,>=1.65.5 (from openpipe-art==0.5.0)\n",
      "  Downloading openai-1.99.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting typer>=0.15.2 (from openpipe-art==0.5.0)\n",
      "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Downloading weave-0.52.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting graphql-core<3.3,>=3.2 (from gql==3.4.1)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.6 (from gql==3.4.1)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting backoff<3.0,>=1.11.1 (from gql==3.4.1)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting aiohttp>=3.10 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading aiohttp-3.13.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting click (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting httpx>=0.23.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/miniforge3/lib/python3.12/site-packages (from litellm==1.74.1->openpipe-art==0.5.0) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/miniforge3/lib/python3.12/site-packages (from litellm==1.74.1->openpipe-art==0.5.0) (3.1.6)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic<3.0.0,>=2.5.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading pydantic-2.12.0-py3-none-any.whl.metadata (83 kB)\n",
      "Collecting python-dotenv>=0.2.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tiktoken>=0.7.0 (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tokenizers in /opt/miniforge3/lib/python3.12/site-packages (from litellm==1.74.1->openpipe-art==0.5.0) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniforge3/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.74.1->openpipe-art==0.5.0) (3.0.3)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading rpds_py-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniforge3/lib/python3.12/site-packages (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Downloading jiter-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /opt/miniforge3/lib/python3.12/site-packages (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniforge3/lib/python3.12/site-packages (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniforge3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/miniforge3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.74.1->openpipe-art==0.5.0) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.23.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.1 (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading pydantic_core-2.41.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.5.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting multidict>=4.0 (from yarl<2.0,>=1.6->gql==3.4.1)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.1 (from yarl<2.0,>=1.6->gql==3.4.1)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core)\n",
      "  Downloading langsmith-0.4.34-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/miniforge3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /opt/miniforge3/lib/python3.12/site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/miniforge3/lib/python3.12/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniforge3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/miniforge3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: filelock in /opt/miniforge3/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/miniforge3/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/miniforge3/lib/python3.12/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniforge3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: regex in /opt/miniforge3/lib/python3.12/site-packages (from vllm) (2025.9.18)\n",
      "Collecting cachetools (from vllm)\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: psutil in /opt/miniforge3/lib/python3.12/site-packages (from vllm) (7.1.0)\n",
      "Collecting sentencepiece (from vllm)\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading blake3-1.0.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (217 bytes)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting protobuf (from vllm)\n",
      "  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi-0.119.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting prometheus_client>=0.18.0 (from vllm)\n",
      "  Downloading prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pillow (from vllm)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting lm-format-enforcer==0.11.3 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
      "  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.11 (from vllm)\n",
      "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.25 (from vllm)\n",
      "  Downloading xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /opt/miniforge3/lib/python3.12/site-packages (from vllm) (27.1.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm)\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm)\n",
      "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/miniforge3/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Collecting setuptools<80,>=77.0.3 (from vllm)\n",
      "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.11.0 (from vllm)\n",
      "  Downloading compressed_tensors-0.11.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.19.0 (from vllm)\n",
      "  Downloading depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting python-json-logger (from vllm)\n",
      "  Downloading python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting scipy (from vllm)\n",
      "  Downloading scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting ninja (from vllm)\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm)\n",
      "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm)\n",
      "  Downloading cbor2-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting setproctitle (from vllm)\n",
      "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm)\n",
      "  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading ray-2.50.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting torchaudio==2.8.0 (from vllm)\n",
      "  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting torchvision==0.23.0 (from vllm)\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.32.post1 (from vllm)\n",
      "  Downloading xformers-0.0.32.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniforge3/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/miniforge3/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: frozendict in /opt/miniforge3/lib/python3.12/site-packages (from compressed-tensors==0.11.0->vllm) (2.4.6)\n",
      "Collecting astor (from depyf==0.19.0->vllm)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.37.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /opt/miniforge3/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniforge3/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniforge3/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniforge3/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/miniforge3/lib/python3.12/site-packages (from peft) (1.10.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.10->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting starlette<0.49.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading starlette-0.48.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cli-0.0.13-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich_toolkit-0.15.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cloud_cli-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rignore-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentry-sdk>=2.20.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading sentry_sdk-2.41.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/miniforge3/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm==1.74.1->openpipe-art==0.5.0) (3.23.0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.41.1-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n",
      "  Downloading pydantic_extra_types-2.10.6-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting click (from litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading cupy_cuda12x-13.6.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/miniforge3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.15.2->openpipe-art==0.5.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting eval-type-backport (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting polyfile-weave (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading polyfile_weave-0.5.7-py3-none-any.whl.metadata (7.6 kB)\n",
      "INFO: pip is looking at multiple versions of weave to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Downloading weave-0.52.8-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /opt/miniforge3/lib/python3.12/site-packages (from weave>=0.51.51->openpipe-art==0.5.0) (1.6.0)\n",
      "Collecting wandb>=0.17.1 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading wandb-0.22.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/miniforge3/lib/python3.12/site-packages (from wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0) (4.3.8)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "INFO: pip is looking at multiple versions of gql[aiohttp,requests] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting frozendict (from compressed-tensors==0.11.0->vllm)\n",
      "  Downloading frozendict-2.4.6-py312-none-any.whl.metadata (23 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting eval-type-backport (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading eval_type_backport-0.2.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading fastrlock-0.8.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading cupy_cuda12x-13.5.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-9.2-py2.py3-none-any.whl.metadata (8.8 kB)\n",
      "INFO: pip is still looking at multiple versions of gql[aiohttp,requests] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting cbor2 (from vllm)\n",
      "  Downloading cbor2-5.6.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading blake3-1.0.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (217 bytes)\n",
      "Collecting astor (from depyf==0.19.0->vllm)\n",
      "  Downloading astor-0.8.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core)\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm==1.74.1->openpipe-art==0.5.0)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.17.1->weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting wandb>=0.17.1 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading wandb-0.22.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting nest-asyncio==1.6.0 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Downloading weave-0.52.7-py3-none-any.whl.metadata (27 kB)\n",
      "  Downloading weave-0.52.6-py3-none-any.whl.metadata (27 kB)\n",
      "  Downloading weave-0.52.5-py3-none-any.whl.metadata (27 kB)\n",
      "  Downloading weave-0.52.4-py3-none-any.whl.metadata (27 kB)\n",
      "  Downloading weave-0.52.3-py3-none-any.whl.metadata (27 kB)\n",
      "  Downloading weave-0.52.2-py3-none-any.whl.metadata (27 kB)\n",
      "INFO: pip is still looking at multiple versions of weave to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading weave-0.52.1-py3-none-any.whl.metadata (27 kB)\n",
      "  Downloading weave-0.51.59-py3-none-any.whl.metadata (26 kB)\n",
      "  Downloading weave-0.51.56-py3-none-any.whl.metadata (26 kB)\n",
      "  Downloading weave-0.51.55-py3-none-any.whl.metadata (26 kB)\n",
      "  Downloading weave-0.51.54-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting emoji>=2.12.1 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Downloading weave-0.51.53-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uuid-utils>=0.9.0 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading uuid_utils-0.11.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting weave>=0.51.51 (from openpipe-art==0.5.0)\n",
      "  Downloading weave-0.51.52-py3-none-any.whl.metadata (25 kB)\n",
      "  Downloading weave-0.51.51-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-1.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-1.0.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-1.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-1.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading watchfiles-0.18.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting emoji>=2.12.1 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-0.18.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting uuid-utils>=0.9.0 (from weave>=0.51.51->openpipe-art==0.5.0)\n",
      "  Downloading uuid_utils-0.11.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Downloading anyio-3.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-0.17.0-cp37-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<=1.99.1,>=1.65.5->openpipe-art==0.5.0)\n",
      "  Downloading anyio-3.6.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-0.16.1-cp37-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading uvloop-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading uvloop-0.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "  Downloading uvloop-0.17.0.tar.gz (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading uvloop-0.16.0.tar.gz (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading uvloop-0.15.3.tar.gz (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading uvloop-0.15.2.tar.gz (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "  Downloading httptools-0.6.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpipe-art==0.5.0 langchain-core tenacity datasets vllm faiss-cpu chromadb requests lxml numpy transformers torch gql==3.4.1 peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965eaa9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'secretsConfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msecretsConfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m oaiKey, wandbKey  \u001b[38;5;66;03m# Import the variables\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Required for RULER judge model\u001b[39;00m\n\u001b[32m      5\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = oaiKey\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'secretsConfig'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from secretsConfig import oaiKey, wandbKey  # Import the variables\n",
    "\n",
    "# Required for RULER judge model\n",
    "os.environ[\"OPENAI_API_KEY\"] = oaiKey\n",
    "\n",
    "# Required for Weights & Biases\n",
    "os.environ[\"WANDB_API_KEY\"] = wandbKey\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY is required for RULER functionality when using openai/o4-mini.\"\n",
    "    )\n",
    "\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY is required for inference, training, and logging to Weights & Biases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IBM_Z_Datathon_RAG.semantic_search import FAISSSemanticSearch\n",
    "from IBM_Z_Datathon_RAG.KeywordSearch import keyword_search\n",
    "from IBM_Z_Datathon_RAG.ReadDocumentPart import read_document_part\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "import art\n",
    "from art.serverless.backend import ServerlessBackend\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Declare the model - CHANGED TO QWEN3-14B\n",
    "model = art.TrainableModel(\n",
    "    name=\"legal-agent-001\",\n",
    "    project=\"legal-rag\",\n",
    "    base_model=\"Qwen/Qwen2.5-14B-Instruct\",  # Changed from Qwen2.5-14B-Instruct\n",
    ")\n",
    "\n",
    "# Initialize the server\n",
    "# Training and inference will run on Weights & Biases servers\n",
    "backend = ServerlessBackend()\n",
    "\n",
    "# Register the model with the Serverless Backend (sets up logging, inference, and training)\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e83a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 196\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# LOCAL TRAINING - uses YOUR A100s\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, TaskType\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m    199\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-14B-Instruct\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'peft'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AsyncOpenAI\n",
    "from litellm import acompletion\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# Your tool imports\n",
    "from IBM_Z_Datathon_RAG.semantic_search import FAISSSemanticSearch\n",
    "from IBM_Z_Datathon_RAG.KeywordSearch import keyword_search\n",
    "from IBM_Z_Datathon_RAG.ReadDocumentPart import read_document_part\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Please set OPENROUTER_API_KEY in your .env file\")\n",
    "if not WANDB_API_KEY:\n",
    "    raise ValueError(\"Please set WANDB_API_KEY in your .env file\")\n",
    "\n",
    "# Login to W&B\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "MAX_TURNS = 4\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    answer: str = Field(description=\"The final answer to the legal question\")\n",
    "    source_ids: list[str] = Field(description=\"List of part IDs used as sources\")\n",
    "\n",
    "\n",
    "class LegalScenario(BaseModel):\n",
    "    id: str\n",
    "    question: str\n",
    "    gold_answer: str | None = None\n",
    "    gold_doc_ids: list[str] | None = None\n",
    "    gold_part_ids: list[str] | None = None\n",
    "\n",
    "\n",
    "class CorrectnessJudgeResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"Explanation of the reasoning process.\")\n",
    "    verdict: str = Field(description=\"correct, incorrect, or idk\")\n",
    "    reward: float = Field(description=\"Numeric reward value\")\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3))\n",
    "async def judge_correctness(\n",
    "    scenario: LegalScenario, \n",
    "    answer: str,\n",
    "    sources: list[str],\n",
    "    trajectory_info: dict\n",
    ") -> CorrectnessJudgeResponse:\n",
    "    \"\"\"Judge using Gemini 2.5 Flash via OpenRouter\"\"\"\n",
    "    \n",
    "    system_prompt = dedent(\n",
    "        \"\"\"\n",
    "        You are evaluating a legal research agent's answer. Grade based on:\n",
    "        \n",
    "        **Correctness (primary):**\n",
    "        - Does the answer correctly address the legal question?\n",
    "        - Does it match the reference answer's key legal points?\n",
    "        \n",
    "        **Sources (grounding):**\n",
    "        - Are the cited source part_ids relevant and correct?\n",
    "        - Did the agent read the right documents?\n",
    "        \n",
    "        **Efficiency:**\n",
    "        - Did the agent find the answer efficiently (fewer turns/searches = better)?\n",
    "        \n",
    "        Return verdict as: \"correct\", \"incorrect\", or \"idk\" (if agent correctly said \"I don't know\")\n",
    "        \n",
    "        Reward scale:\n",
    "        - Correct answer: +1.0 to +2.3\n",
    "        - I don't know (when appropriate): 0.0 to +0.8\n",
    "        - Wrong answer: -1.0 to 0.0\n",
    "        - Formatting errors: -2.0 to -1.0\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Question: {scenario.question}\\n\\n\"\n",
    "                f\"Reference answer: {scenario.gold_answer}\\n\\n\"\n",
    "                f\"Agent's answer: {answer}\\n\\n\"\n",
    "                f\"Agent's cited sources: {sources}\\n\\n\"\n",
    "                f\"Gold part IDs: {scenario.gold_part_ids}\\n\\n\"\n",
    "                f\"Trajectory info:\\n\"\n",
    "                f\"- Number of turns: {trajectory_info.get('num_turns', 0)}\\n\"\n",
    "                f\"- Number of searches: {trajectory_info.get('num_searches', 0)}\\n\"\n",
    "                f\"- Found right doc: {trajectory_info.get('found_right_doc', False)}\\n\"\n",
    "                f\"- Read right part: {trajectory_info.get('read_right_part', False)}\\n\\n\"\n",
    "                \"Provide your evaluation with reasoning, verdict, and numeric reward.\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = await acompletion(\n",
    "        model=\"google/gemini-2.5-flash\",\n",
    "        messages=messages,\n",
    "        response_format=CorrectnessJudgeResponse,\n",
    "        api_base=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "    )\n",
    "\n",
    "    first_choice = response.choices[0]\n",
    "    raw_content = first_choice.message.content or \"{}\"\n",
    "\n",
    "    try:\n",
    "        return CorrectnessJudgeResponse.model_validate_json(raw_content)\n",
    "    except Exception as e:\n",
    "        return CorrectnessJudgeResponse(\n",
    "            reasoning=f\"Parse error: {e}\",\n",
    "            verdict=\"incorrect\",\n",
    "            reward=-2.0\n",
    "        )\n",
    "\n",
    "\n",
    "class Trajectory:\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.final_answer: FinalAnswer | None = None\n",
    "        self.reward: float = 0.0\n",
    "        self.num_turns: int = 0\n",
    "        self.num_searches: int = 0\n",
    "        self.found_right_doc: bool = False\n",
    "        self.read_right_part: bool = False\n",
    "        self.metadata: dict = {}\n",
    "\n",
    "\n",
    "async def rollout(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    scenario: LegalScenario,\n",
    "    step: int\n",
    ") -> Trajectory:\n",
    "    \"\"\"Execute one trajectory rollout\"\"\"\n",
    "    \n",
    "    traj = Trajectory()\n",
    "    traj.metadata = {\n",
    "        \"scenario_id\": scenario.id,\n",
    "        \"step\": step,\n",
    "    }\n",
    "\n",
    "    system_prompt = dedent(\n",
    "        f\"\"\"\n",
    "        You are a legal research agent with access to tools for searching legal documents.\n",
    "        \n",
    "        Available tools:\n",
    "        - search_keyword(query, num): Exact term matching (BM25)\n",
    "        - search_semantic(query, num): Semantic/conceptual search\n",
    "        - read_document_part(part_id): Read full text of a document part\n",
    "        - return_final_answer(answer, source_ids): Provide final answer with citations\n",
    "        \n",
    "        Rules:\n",
    "        1. You have up to {MAX_TURNS} turns to find the answer\n",
    "        2. Use tools strategically - start broad, then narrow down\n",
    "        3. ALWAYS cite your sources using part_ids\n",
    "        4. If you cannot find sufficient information, say \"I don't know\" rather than guessing\n",
    "        \n",
    "        Format your tool calls as:\n",
    "        <tool>{{\"name\": \"tool_name\", \"args\": {{\"param\": \"value\"}}}}</tool>\n",
    "        \n",
    "        Format your final answer as:\n",
    "        <answer>Your answer here</answer>\n",
    "        <sources><source>part_id_1</source><source>part_id_2</source></sources>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    traj.messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": scenario.question},\n",
    "    ]\n",
    "\n",
    "    # Define tools\n",
    "    def search_keyword_tool(query: str, num: int = 5) -> str:\n",
    "        \"\"\"Search using keyword/BM25 for exact term matches.\"\"\"\n",
    "        traj.num_searches += 1\n",
    "        return keyword_search(query, num)\n",
    "\n",
    "    def search_semantic_tool(query: str, num: int = 5) -> str:\n",
    "        \"\"\"Search using semantic/vector search.\"\"\"\n",
    "        traj.num_searches += 1\n",
    "        searcher = FAISSSemanticSearch()\n",
    "        return searcher.search(query, num)\n",
    "\n",
    "    def read_document_part_tool(part_id: str) -> str:\n",
    "        \"\"\"Read a document part by ID.\"\"\"\n",
    "        result = read_document_part(part_id)\n",
    "        \n",
    "        # Check if we read the right part\n",
    "        if scenario.gold_part_ids and part_id in scenario.gold_part_ids:\n",
    "            traj.read_right_part = True\n",
    "        \n",
    "        # Check if we found the right document\n",
    "        if scenario.gold_doc_ids:\n",
    "            doc_id = part_id.split(':')[0]\n",
    "            if doc_id in scenario.gold_doc_ids:\n",
    "                traj.found_right_doc = True\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def return_final_answer(answer: str, source_ids: list[str]) -> FinalAnswer:\n",
    "        \"\"\"Return final answer with sources.\"\"\"\n",
    "        return FinalAnswer(answer=answer, source_ids=source_ids)\n",
    "\n",
    "    tools = [\n",
    "        search_keyword_tool,\n",
    "        search_semantic_tool,\n",
    "        read_document_part_tool,\n",
    "        return_final_answer\n",
    "    ]\n",
    "    tools_by_name = {t.__name__: t for t in tools}\n",
    "\n",
    "    # Agent loop (simplified - you'll implement full generation)\n",
    "    for turn in range(MAX_TURNS):\n",
    "        traj.num_turns = turn + 1\n",
    "        \n",
    "        # TODO: Implement proper tool-calling generation here\n",
    "        # For now this is a placeholder\n",
    "        \n",
    "        if traj.final_answer:\n",
    "            break\n",
    "    \n",
    "    # Calculate reward using Gemini judge\n",
    "    if traj.final_answer:\n",
    "        trajectory_info = {\n",
    "            \"num_turns\": traj.num_turns,\n",
    "            \"num_searches\": traj.num_searches,\n",
    "            \"found_right_doc\": traj.found_right_doc,\n",
    "            \"read_right_part\": traj.read_right_part,\n",
    "        }\n",
    "        \n",
    "        judge_response = await judge_correctness(\n",
    "            scenario,\n",
    "            traj.final_answer.answer,\n",
    "            traj.final_answer.source_ids,\n",
    "            trajectory_info\n",
    "        )\n",
    "        \n",
    "        traj.reward = judge_response.reward\n",
    "        \n",
    "        # Log to W&B\n",
    "        wandb.log({\n",
    "            \"reward\": traj.reward,\n",
    "            \"verdict\": judge_response.verdict,\n",
    "            \"num_turns\": traj.num_turns,\n",
    "            \"num_searches\": traj.num_searches,\n",
    "            \"found_right_doc\": traj.found_right_doc,\n",
    "            \"read_right_part\": traj.read_right_part,\n",
    "        })\n",
    "    else:\n",
    "        traj.reward = -1.0\n",
    "        wandb.log({\"reward\": traj.reward, \"status\": \"no_answer\"})\n",
    "    \n",
    "    return traj\n",
    "\n",
    "\n",
    "# Main training function\n",
    "async def train_grpo():\n",
    "    \"\"\"Main GRPO training loop with W&B logging\"\"\"\n",
    "    \n",
    "    # Initialize W&B run\n",
    "    config = {\n",
    "        \"model\": \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "        \"lora_r\": 32,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"max_turns\": MAX_TURNS,\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": 1e-5,\n",
    "    }\n",
    "    \n",
    "    run = wandb.init(\n",
    "        project=\"legal-rag-grpo\",\n",
    "        config=config,\n",
    "        name=\"qwen2.5-14b-lora-rl\"\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model_name = config[\"model\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    print(\"Applying LoRA...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"lora_r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Load scenarios\n",
    "    print(\"Loading scenarios...\")\n",
    "    scenarios = []\n",
    "    # TODO: Load your JSON data here\n",
    "    \n",
    "    print(f\"✅ Setup complete! Training on {len(scenarios)} scenarios\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{config['num_epochs']} ===\")\n",
    "        \n",
    "        epoch_rewards = []\n",
    "        \n",
    "        for i, scenario in enumerate(scenarios):\n",
    "            # Run trajectory\n",
    "            traj = await rollout(model, tokenizer, scenario, step=i)\n",
    "            epoch_rewards.append(traj.reward)\n",
    "            \n",
    "            # TODO: Update model with GRPO based on trajectory\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                avg_reward = sum(epoch_rewards) / len(epoch_rewards)\n",
    "                print(f\"Batch {i + 1}/{len(scenarios)} | Avg Reward: {avg_reward:.3f}\")\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        avg_epoch_reward = sum(epoch_rewards) / len(epoch_rewards)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"epoch_avg_reward\": avg_epoch_reward,\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} | Avg Reward: {avg_epoch_reward:.3f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model.save_pretrained(\"./legal-rag-lora-final\")\n",
    "    wandb.save(\"./legal-rag-lora-final/*\")\n",
    "    \n",
    "    run.finish()\n",
    "    print(\"✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(train_grpo())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
