{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3873e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpipe-art==0.5.0 langchain-core tenacity datasets vllm faiss-cpu chromadb requests lxml numpy transformers torch gql==3.4.1 peft \n",
    "# !pip install langchain-core tenacity datasets vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965eaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from secretsConfig import oaiKey, wandbKey, openRouterKey  # Add openRouterKey\n",
    "\n",
    "# Required for RULER judge model\n",
    "os.environ[\"OPENAI_API_KEY\"] = oaiKey\n",
    "\n",
    "# Required for Weights & Biases\n",
    "os.environ[\"WANDB_API_KEY\"] = wandbKey\n",
    "\n",
    "# Required for OpenRouter (Gemini judge)\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = openRouterKey  # ADD THIS LINE\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY is required for RULER functionality.\")\n",
    "\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY is required for W&B.\")\n",
    "\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    raise ValueError(\"OPENROUTER_API_KEY is required for Gemini judge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6c3f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tools wrapped with error handling and validation\n"
     ]
    }
   ],
   "source": [
    "from IBM_Z_Datathon_RAG.semantic_search import FAISSSemanticSearch\n",
    "from IBM_Z_Datathon_RAG.KeywordSearch import keyword_search\n",
    "from IBM_Z_Datathon_RAG.ReadDocumentPart import read_document_part\n",
    "\n",
    "# Wrap tools with error handling that tracks mistakes\n",
    "class ToolError:\n",
    "    \"\"\"Marker for tool execution errors\"\"\"\n",
    "    def __init__(self, message: str):\n",
    "        self.message = message\n",
    "\n",
    "_original_keyword_search = keyword_search\n",
    "_original_read_document_part = read_document_part\n",
    "\n",
    "def keyword_search(query: str, num: int = 5) -> str:\n",
    "    \"\"\"Safe keyword search wrapper\"\"\"\n",
    "    try:\n",
    "        return _original_keyword_search(query, num)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"[TOOL ERROR] keyword_search failed: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "def read_document_part(part_id: str) -> str:\n",
    "    \"\"\"Safe read_document_part wrapper - validates part_id format\"\"\"\n",
    "    try:\n",
    "        # Validate part_id format (should be like \"doc:section:pX\", not a search query)\n",
    "        if \" \" in part_id or len(part_id) > 100:\n",
    "            error_msg = f\"[INVALID PART_ID] '{part_id[:50]}...' is not a valid part_id format. Part IDs should be like 'doc:section:pX'. Use search tools first to find valid part IDs.\"\n",
    "            print(f\"[ERROR] Model tried to read invalid part_id: {part_id[:50]}...\")\n",
    "            return error_msg\n",
    "        \n",
    "        return _original_read_document_part(part_id)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        error_msg = f\"[PART NOT FOUND] Part ID '{part_id}' does not exist. Use search_keyword or search_semantic first to find valid part IDs.\"\n",
    "        print(f\"[ERROR] {error_msg}\")\n",
    "        return error_msg\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"[TOOL ERROR] Failed to read document part: {str(e)}\"\n",
    "        print(f\"[ERROR] {error_msg}\")\n",
    "        return error_msg\n",
    "\n",
    "print(\"✅ Tools wrapped with error handling and validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535e659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "import art\n",
    "from art.serverless.backend import ServerlessBackend\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Declare the model - CHANGED TO QWEN3-14B\n",
    "model = art.TrainableModel(\n",
    "    name=\"legal-agent-001\",\n",
    "    project=\"legal-rag\",\n",
    "    base_model=\"Qwen/Qwen2.5-14B-Instruct\",  # Changed from Qwen2.5-14B-Instruct\n",
    ")\n",
    "\n",
    "# Initialize the server\n",
    "# Training and inference will run on Weights & Biases servers\n",
    "backend = ServerlessBackend()\n",
    "\n",
    "# Register the model with the Serverless Backend (sets up logging, inference, and training)\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9e83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rollout function defined!\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AsyncOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "import art\n",
    "\n",
    "MAX_TURNS = 4\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    answer: str\n",
    "    source_ids: list[str]\n",
    "\n",
    "\n",
    "class LegalScenario(BaseModel):\n",
    "    id: str\n",
    "    question: str\n",
    "    gold_answer: str | None = None\n",
    "    gold_part_ids: list[str] | None = None\n",
    "\n",
    "\n",
    "class LegalScenarioStep(BaseModel):\n",
    "    step: int\n",
    "    scenario: LegalScenario\n",
    "\n",
    "\n",
    "async def rollout(model: art.Model, legal_scenario_step: LegalScenarioStep) -> art.Trajectory:\n",
    "    \"\"\"Execute one trajectory rollout\"\"\"\n",
    "    scenario = legal_scenario_step.scenario\n",
    "    \n",
    "    traj = art.Trajectory(\n",
    "        reward=0.0,\n",
    "        messages_and_choices=[],\n",
    "        metadata={\"scenario_id\": scenario.id, \"step\": legal_scenario_step.step},\n",
    "    )\n",
    "\n",
    "    # YOUR CUSTOM PROMPT HERE\n",
    "    system_prompt = dedent(\n",
    "        f\"\"\"\n",
    "        You are a legal research assistant that can search legal documents to answer questions.\n",
    "\n",
    "        You have access to the following tools:\n",
    "\n",
    "        - search_keyword(query: str, num: int) -> str: Search using keyword/BM25 search for exact term matches.\n",
    "        - search_semantic(query: str, num: int) -> str: Search using semantic/vector search for conceptual similarity.\n",
    "        - read_document_part(part_id: str) -> str: Read a document part by ID. Part IDs use hierarchical format (e.g., A:B:C). To access parent parts, remove the last segment (e.g., A:B:C → parent is A:B).\n",
    "\n",
    "        You may call one tool per turn, for up to {MAX_TURNS} turns, before giving your final answer.\n",
    "\n",
    "        In each turn, you should analyze what information you need and respond with EITHER a tool call OR your final answer.\n",
    "\n",
    "        For tool calls, use this format:\n",
    "        <think>\n",
    "        [your reasoning for what to search for and why]\n",
    "        </think>\n",
    "        <tool>\n",
    "        {{\"name\": \"tool_name\", \"args\": {{\"query\": \"search query\"}}}}\n",
    "        </tool>\n",
    "\n",
    "        When you have enough information, give your final answer in this format:\n",
    "\n",
    "        <think>\n",
    "        [your reasoning for the answer]\n",
    "        </think>\n",
    "        <answer>\n",
    "        [your comprehensive answer citing the evidence you found or \"I don't know\" if you didn't get enough information]\n",
    "\n",
    "        <sources>\n",
    "        <source>doc_id_1</source>\n",
    "        </sources>\n",
    "        </answer>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    traj.messages_and_choices = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": scenario.question},\n",
    "    ]\n",
    "\n",
    "    # Define tools\n",
    "    def search_keyword_tool(query: str, num: int = 5) -> str:\n",
    "        return keyword_search(query, num)\n",
    "\n",
    "    def search_semantic_tool(query: str, num: int = 5) -> str:\n",
    "        searcher = FAISSSemanticSearch()\n",
    "        return searcher.search(query, num)\n",
    "\n",
    "    def read_document_part_tool(part_id: str) -> str:\n",
    "        return read_document_part(part_id)\n",
    "\n",
    "    def return_final_answer(answer: str, source_ids: list[str]) -> FinalAnswer:\n",
    "        return FinalAnswer(answer=answer, source_ids=source_ids)\n",
    "\n",
    "    tools = [search_keyword_tool, search_semantic_tool, read_document_part_tool, return_final_answer]\n",
    "    tools_by_name = {t.__name__: t for t in tools}\n",
    "    traj.tools = [convert_to_openai_tool(t) for t in tools]\n",
    "\n",
    "    client = AsyncOpenAI(\n",
    "        base_url=model.inference_base_url,\n",
    "        api_key=model.inference_api_key,\n",
    "    )\n",
    "\n",
    "    for _ in range(MAX_TURNS):\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model.get_inference_name(),\n",
    "            temperature=1,\n",
    "            messages=traj.messages(),\n",
    "            tools=traj.tools,\n",
    "        )\n",
    "\n",
    "        response_message = response.choices[0].message\n",
    "        traj.messages_and_choices.append(response.choices[0])\n",
    "\n",
    "        if not response_message.tool_calls:\n",
    "            return traj\n",
    "\n",
    "        try:\n",
    "            for tool_call in response_message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                if tool_name in tools_by_name:\n",
    "                    tool_args = json.loads(tool_call.function.arguments)\n",
    "                    result = tools_by_name[tool_name](**tool_args)\n",
    "                    traj.messages_and_choices.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(result),\n",
    "                    })\n",
    "\n",
    "                    if tool_name == \"return_final_answer\":\n",
    "                        return traj\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return traj\n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "print(\"✅ Rollout function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055f5c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./snippet_data.json...\n",
      "✅ Loaded 100 scenarios\n",
      "\n",
      "🧪 Testing Gemini judge via OpenRouter...\n",
      "  Scores: [2.0, 0.0]\n",
      "\n",
      "Rank 1: Score 2.000\n",
      "  Response: The Marshall Court reasoned that a land grant from a state constitutes a binding...\n",
      "\n",
      "Rank 2: Score 0.000\n",
      "  Response: I don't know anything about this legal question....\n",
      "\n",
      "✅ Gemini judge working!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from litellm import acompletion\n",
    "\n",
    "# Load your training data\n",
    "DATA_FILE = \"./snippet_data.json\"\n",
    "\n",
    "print(f\"Loading data from {DATA_FILE}...\")\n",
    "with open(DATA_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to LegalScenario objects\n",
    "training_scenarios = []\n",
    "for item in data.get(\"items\", []):\n",
    "    for row in item.get(\"rows\", []):\n",
    "        sources = row.get(\"sources\", [])\n",
    "        gold_part_ids = sources if sources else []\n",
    "        \n",
    "        training_scenarios.append(\n",
    "            LegalScenario(\n",
    "                id=str(row[\"row_index\"]),\n",
    "                question=row[\"question\"],\n",
    "                gold_answer=row.get(\"model_answer\", \"\"),\n",
    "                gold_part_ids=gold_part_ids\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"✅ Loaded {len(training_scenarios)} scenarios\")\n",
    "\n",
    "\n",
    "# Custom RULER function using OpenRouter\n",
    "async def gemini_ruler_score_group(group: art.TrajectoryGroup) -> art.TrajectoryGroup:\n",
    "    \"\"\"Score trajectories using Gemini 2.5 Flash via OpenRouter\"\"\"\n",
    "    \n",
    "    trajectories = group.trajectories\n",
    "    if len(trajectories) <= 1:\n",
    "        for traj in trajectories:\n",
    "            traj.reward = 0.0\n",
    "        return group\n",
    "    \n",
    "    # Extract responses\n",
    "    responses = []\n",
    "    for traj in trajectories:\n",
    "        messages = traj.messages()\n",
    "        if messages:\n",
    "            last_msg = messages[-1].get(\"content\", \"\")\n",
    "            responses.append(last_msg)\n",
    "        else:\n",
    "            responses.append(\"\")\n",
    "    \n",
    "    # Build comparison prompt\n",
    "    comparison_text = \"\\n\\n\".join([\n",
    "        f\"**Response {i+1}:**\\n{resp[:500]}\"  # Truncate for API limits\n",
    "        for i, resp in enumerate(responses)\n",
    "    ])\n",
    "    \n",
    "    judge_prompt = f\"\"\"Compare these {len(responses)} legal research responses and rank them.\n",
    "\n",
    "Criteria:\n",
    "1. Correctness and accuracy (most important)\n",
    "2. Proper citation of sources with part_ids\n",
    "3. Completeness of answer\n",
    "\n",
    "Responses:\n",
    "{comparison_text}\n",
    "\n",
    "Return ONLY a JSON array of scores from 0.0 to 2.0, one score per response in order.\n",
    "Higher scores = better responses.\n",
    "Example: [2.0, 0.5, 1.5]\n",
    "\n",
    "Your scores:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Call Gemini via OpenRouter (correct format)\n",
    "        response = await acompletion(\n",
    "        model=\"openrouter/google/gemini-2.5-flash\",  # openrouter/ prefix\n",
    "        messages=[{\"role\": \"user\", \"content\": judge_prompt}],\n",
    "        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "        \n",
    "        # Parse scores\n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Extract JSON array\n",
    "        import re\n",
    "        json_match = re.search(r'\\[[\\d\\.,\\s]+\\]', result_text)\n",
    "        if json_match:\n",
    "            scores = json.loads(json_match.group())\n",
    "        else:\n",
    "            scores = json.loads(result_text)\n",
    "        \n",
    "        # Assign scores\n",
    "        for traj, score in zip(trajectories, scores):\n",
    "            traj.reward = float(score)\n",
    "        \n",
    "        print(f\"  Scores: {scores}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error in judge: {e}\")\n",
    "        # Fallback: random variation\n",
    "        import random\n",
    "        for traj in trajectories:\n",
    "            traj.reward = random.uniform(0.5, 1.5)\n",
    "    \n",
    "    return group\n",
    "\n",
    "\n",
    "# Test the judge\n",
    "print(\"\\n🧪 Testing Gemini judge via OpenRouter...\")\n",
    "\n",
    "test_scenario = training_scenarios[0]\n",
    "base_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a legal research agent.\"},\n",
    "    {\"role\": \"user\", \"content\": test_scenario.question},\n",
    "]\n",
    "\n",
    "good_traj = art.Trajectory(\n",
    "    messages_and_choices=[\n",
    "        *base_messages,\n",
    "        {\"role\": \"assistant\", \"content\": test_scenario.gold_answer},\n",
    "    ],\n",
    "    reward=0,\n",
    ")\n",
    "\n",
    "bad_traj = art.Trajectory(\n",
    "    messages_and_choices=[\n",
    "        *base_messages,\n",
    "        {\"role\": \"assistant\", \"content\": \"I don't know anything about this legal question.\"},\n",
    "    ],\n",
    "    reward=0,\n",
    ")\n",
    "\n",
    "test_group = art.TrajectoryGroup(trajectories=[good_traj, bad_traj])\n",
    "\n",
    "# Score using custom function\n",
    "judged_group = await gemini_ruler_score_group(test_group)\n",
    "\n",
    "# Display results\n",
    "sorted_trajs = sorted(judged_group.trajectories, key=lambda t: t.reward, reverse=True)\n",
    "for rank, traj in enumerate(sorted_trajs, 1):\n",
    "    msgs = traj.messages()\n",
    "    print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
    "    print(f\"  Response: {msgs[-1]['content'][:80]}...\")\n",
    "\n",
    "print(\"\\n✅ Gemini judge working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshng2025\u001b[0m (\u001b[33mImperial-College-London-SPQR\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/maindir/wandb/run-20251012_003057-6q6qwhwv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025/runs/6q6qwhwv' target=\"_blank\">honest-plasma-9</a></strong> to <a href='https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025' target=\"_blank\">https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025/runs/6q6qwhwv' target=\"_blank\">https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025/runs/6q6qwhwv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [litellm, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training loop...\n",
      "\n",
      "💻 Model inference running on: W&B Serverless (not your A100s)\n",
      "📊 W&B Dashboard: https://wandb.ai/Imperial-College-London-SPQR/IBM-Datathon-Z-2025/runs/6q6qwhwv\n",
      "🏷️  Run name: honest-plasma-9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating dataset:   0%|          | 0/150 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 0 | Epoch 0 | Epoch Step 0 ===\n",
      "Batch: 2 scenarios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering trajectories: 100%|██████████| 12/12 [00:02<00:00,  5.55it/s, reward=0, completion_tokens=83.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scores: [1.5, 1.5, 1.5, 1.5, 1.5, 0.5]\n",
      "  Scores: [0.5, 1.0, 1.5, 0.0, 1.0, 1.0]\n",
      "  Rewards: avg=1.083, max=1.500, min=0.000\n",
      "  Correct: 9, IDK: 3, Wrong: 0, Format Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 2/2 [00:16<00:00,  8.33s/it, entropy=0.352, grad_norm=0.659, loss=0.361, policy_loss=0.361]\n",
      "Iterating dataset:   1%|          | 1/150 [00:24<1:01:18, 24.69s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 0 complete\n",
      "\n",
      "=== Step 1 | Epoch 0 | Epoch Step 1 ===\n",
      "Batch: 2 scenarios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering trajectories:  83%|████████▎ | 10/12 [00:04<00:00,  2.32it/s, exceptions=2, reward=0, completion_tokens=84.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scores: [1.0, 1.0, 1.5, 1.5, 2.0, 1.0]\n",
      "  Scores: [1.5, 1.8, 2.0, 1.0]\n",
      "  Rewards: avg=1.430, max=2.000, min=1.000\n",
      "  Correct: 10, IDK: 0, Wrong: 0, Format Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 2/2 [00:17<00:00,  8.64s/it, entropy=0.263, grad_norm=0.194, loss=-0.153, policy_loss=-0.153]\n",
      "Iterating dataset:   1%|▏         | 2/150 [00:55<1:10:08, 28.44s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1 complete\n",
      "\n",
      "=== Step 2 | Epoch 0 | Epoch Step 2 ===\n",
      "Batch: 2 scenarios\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from art.utils import iterate_dataset\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize W&B with auto-generated run name\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "run = wandb.init(\n",
    "    project=\"IBM-Datathon-Z-2025\",\n",
    "    config={\n",
    "        \"model\": \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "        \"groups_per_step\": 2,\n",
    "        \"num_epochs\": 3,\n",
    "        \"rollouts_per_group\": 6,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"max_steps\": 50,\n",
    "        \"max_turns\": MAX_TURNS,\n",
    "    },\n",
    "    # Remove the name parameter to get auto-generated names with numbers\n",
    "    # name=\"legal-rag-rl-training\"  # REMOVED THIS LINE\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_config = run.config\n",
    "\n",
    "# Create training iterator starting from step 0 (fresh run)\n",
    "training_iterator = iterate_dataset(\n",
    "    training_scenarios,\n",
    "    groups_per_step=training_config[\"groups_per_step\"],\n",
    "    num_epochs=training_config[\"num_epochs\"],\n",
    "    initial_step=0,  # CHANGED: Always start from 0 for fresh runs\n",
    ")\n",
    "\n",
    "print(\"🚀 Starting training loop...\\n\")\n",
    "print(f\"💻 Model inference running on: W&B Serverless (not your A100s)\")\n",
    "print(f\"📊 W&B Dashboard: {run.url}\")\n",
    "print(f\"🏷️  Run name: {run.name}\\n\")\n",
    "\n",
    "step_count = 0\n",
    "\n",
    "for batch in training_iterator:\n",
    "    print(f\"=== Step {batch.step} | Epoch {batch.epoch} | Epoch Step {batch.epoch_step} ===\")\n",
    "    print(f\"Batch: {len(batch.items)} scenarios\")\n",
    "    \n",
    "    # Create trajectory groups\n",
    "    groups = []\n",
    "    for scenario in batch.items:\n",
    "        groups.append(\n",
    "            art.TrajectoryGroup(\n",
    "                (\n",
    "                    rollout(model, LegalScenarioStep(step=batch.step, scenario=scenario))\n",
    "                    for _ in range(training_config[\"rollouts_per_group\"])\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Gather trajectories\n",
    "    finished_groups = await art.gather_trajectory_groups(\n",
    "        groups,\n",
    "        pbar_desc=\"Gathering trajectories\",\n",
    "        max_exceptions=training_config[\"rollouts_per_group\"] * len(batch.items),\n",
    "    )\n",
    "    \n",
    "    # Judge with custom Gemini function\n",
    "    judged_groups = []\n",
    "    for group in finished_groups:\n",
    "        judged_group = await gemini_ruler_score_group(group)\n",
    "        judged_groups.append(judged_group)\n",
    "    \n",
    "    # Calculate metrics before training\n",
    "    all_rewards = [t.reward for g in judged_groups for t in g.trajectories]\n",
    "    avg_reward = sum(all_rewards) / len(all_rewards)\n",
    "    max_reward = max(all_rewards)\n",
    "    min_reward = min(all_rewards)\n",
    "    \n",
    "    # Count trajectories by reward band\n",
    "    correct_count = sum(1 for r in all_rewards if r >= 1.0)\n",
    "    idk_count = sum(1 for r in all_rewards if 0.0 <= r < 1.0)\n",
    "    wrong_count = sum(1 for r in all_rewards if -1.0 <= r < 0.0)\n",
    "    format_error_count = sum(1 for r in all_rewards if r < -1.0)\n",
    "    \n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        \"step\": step_count,  # Use step_count instead of batch.step\n",
    "        \"epoch\": batch.epoch,\n",
    "        \"avg_reward\": avg_reward,\n",
    "        \"max_reward\": max_reward,\n",
    "        \"min_reward\": min_reward,\n",
    "        \"correct_count\": correct_count,\n",
    "        \"idk_count\": idk_count,\n",
    "        \"wrong_count\": wrong_count,\n",
    "        \"format_error_count\": format_error_count,\n",
    "        \"total_trajectories\": len(all_rewards),\n",
    "    })\n",
    "    \n",
    "    print(f\"  Rewards: avg={avg_reward:.3f}, max={max_reward:.3f}, min={min_reward:.3f}\")\n",
    "    print(f\"  Correct: {correct_count}, IDK: {idk_count}, Wrong: {wrong_count}, Format Errors: {format_error_count}\")\n",
    "    \n",
    "    # Train on judged trajectories\n",
    "    await model.delete_checkpoints()\n",
    "    await model.train(\n",
    "        judged_groups,\n",
    "        config=art.TrainConfig(learning_rate=training_config[\"learning_rate\"]),\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Step {step_count} complete\\n\")\n",
    "    \n",
    "    step_count += 1\n",
    "    \n",
    "    # Stop after max_steps\n",
    "    if step_count >= training_config[\"max_steps\"]:\n",
    "        break\n",
    "\n",
    "run.finish()\n",
    "print(\"🎉 Training complete!\")\n",
    "print(f\"📊 View results: {run.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b06c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
