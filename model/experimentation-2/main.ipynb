{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpipe-art==0.5.0 langchain-core tenacity datasets vllm faiss-cpu chromadb requests lxml numpy transformers torch gql==3.4.1 peft\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965eaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from secretsConfig import oaiKey, wandbKey  # Import the variables\n",
    "\n",
    "# Required for RULER judge model\n",
    "os.environ[\"OPENAI_API_KEY\"] = oaiKey\n",
    "\n",
    "# Required for Weights & Biases\n",
    "os.environ[\"WANDB_API_KEY\"] = wandbKey\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY is required for RULER functionality when using openai/o4-mini.\"\n",
    "    )\n",
    "\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY is required for inference, training, and logging to Weights & Biases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6c3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IBM_Z_Datathon_RAG.semantic_search import FAISSSemanticSearch\n",
    "from IBM_Z_Datathon_RAG.KeywordSearch import keyword_search\n",
    "from IBM_Z_Datathon_RAG.ReadDocumentPart import read_document_part\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535e659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "import art\n",
    "from art.serverless.backend import ServerlessBackend\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Declare the model - CHANGED TO QWEN3-14B\n",
    "model = art.TrainableModel(\n",
    "    name=\"legal-agent-001\",\n",
    "    project=\"legal-rag\",\n",
    "    base_model=\"Qwen/Qwen2.5-14B-Instruct\",  # Changed from Qwen2.5-14B-Instruct\n",
    ")\n",
    "\n",
    "# Initialize the server\n",
    "# Training and inference will run on Weights & Biases servers\n",
    "backend = ServerlessBackend()\n",
    "\n",
    "# Register the model with the Serverless Backend (sets up logging, inference, and training)\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9e83a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 196\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# LOCAL TRAINING - uses YOUR A100s\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, TaskType\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m    199\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-14B-Instruct\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'peft'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AsyncOpenAI\n",
    "from litellm import acompletion\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from tenacity import retry, stop_after_attempt\n",
    "import art\n",
    "\n",
    "# Your tool imports\n",
    "from IBM_Z_Datathon_RAG.semantic_search import FAISSSemanticSearch\n",
    "from IBM_Z_Datathon_RAG.KeywordSearch import keyword_search\n",
    "from IBM_Z_Datathon_RAG.ReadDocumentPart import read_document_part\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MAX_TURNS = 4\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    answer: str = Field(description=\"The final answer to the legal question\")\n",
    "    source_ids: list[str] = Field(description=\"List of part IDs used as sources\")\n",
    "\n",
    "\n",
    "class LegalScenario(BaseModel):\n",
    "    id: str\n",
    "    question: str\n",
    "    gold_answer: str | None = None\n",
    "    gold_doc_ids: list[str] | None = None\n",
    "    gold_part_ids: list[str] | None = None\n",
    "\n",
    "\n",
    "class CorrectnessJudgeResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"Explanation of the reasoning process.\")\n",
    "    accept: bool = Field(description=\"Whether the AI answer should be accepted.\")\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3))\n",
    "async def judge_correctness(\n",
    "    scenario: LegalScenario, answer: str\n",
    ") -> CorrectnessJudgeResponse:\n",
    "    system_prompt = dedent(\n",
    "        \"\"\"\n",
    "        You are given a legal question, the reference answer, and an AI answer.\n",
    "        Decide whether the AI answer is correct and should be accepted.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Question: {scenario.question}\\n\"\n",
    "                f\"Reference answer: {scenario.gold_answer}\\n\"\n",
    "                f\"AI answer: {answer}\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = await acompletion(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        response_format=CorrectnessJudgeResponse,\n",
    "    )\n",
    "\n",
    "    first_choice = response.choices[0]\n",
    "    raw_content = first_choice.message.content or \"{}\"\n",
    "\n",
    "    try:\n",
    "        return CorrectnessJudgeResponse.model_validate_json(raw_content)\n",
    "    except Exception as e:\n",
    "        return CorrectnessJudgeResponse(\n",
    "            reasoning=f\"Parse error: {e}\", accept=False\n",
    "        )\n",
    "\n",
    "\n",
    "class ProjectTrajectory(art.Trajectory):\n",
    "    final_answer: FinalAnswer | None = None\n",
    "\n",
    "\n",
    "class LegalScenarioStep(BaseModel):\n",
    "    step: int\n",
    "    scenario: LegalScenario\n",
    "\n",
    "\n",
    "async def rollout(model: art.Model, legal_scenario_step: LegalScenarioStep) -> ProjectTrajectory:\n",
    "    \"\"\"This is the rollout function - defines how your agent interacts with tools\"\"\"\n",
    "    scenario = legal_scenario_step.scenario\n",
    "\n",
    "    traj = ProjectTrajectory(\n",
    "        reward=0.0,\n",
    "        messages_and_choices=[],\n",
    "        metadata={\n",
    "            \"scenario_id\": scenario.id,\n",
    "            \"step\": legal_scenario_step.step,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    system_prompt = dedent(\n",
    "        f\"\"\"\n",
    "        You are a legal research agent. Use the available tools to search legal documents and answer the question.\n",
    "        You may take up to {MAX_TURNS} turns.\n",
    "\n",
    "        Tools:\n",
    "        - search_keyword: Exact term matches\n",
    "        - search_semantic: Conceptual searches\n",
    "        - read_document_part: Read full text using part_id\n",
    "\n",
    "        Always cite sources using part_ids.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    traj.messages_and_choices = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": scenario.question},\n",
    "    ]\n",
    "\n",
    "    # Define tools\n",
    "    def search_keyword_tool(query: str, num: int = 5) -> str:\n",
    "        \"\"\"Search using keyword/BM25 for exact term matches.\"\"\"\n",
    "        return keyword_search(query, num)\n",
    "\n",
    "    def search_semantic_tool(query: str, num: int = 5) -> str:\n",
    "        \"\"\"Search using semantic/vector search.\"\"\"\n",
    "        searcher = FAISSSemanticSearch()\n",
    "        return searcher.search(query, num)\n",
    "\n",
    "    def read_document_part_tool(part_id: str) -> str:\n",
    "        \"\"\"Read a document part by ID.\"\"\"\n",
    "        return read_document_part(part_id)\n",
    "\n",
    "    def return_final_answer(answer: str, source_ids: list[str]) -> FinalAnswer:\n",
    "        \"\"\"Return final answer with sources.\"\"\"\n",
    "        return FinalAnswer(answer=answer, source_ids=source_ids)\n",
    "\n",
    "    tools = [search_keyword_tool, search_semantic_tool, read_document_part_tool, return_final_answer]\n",
    "    tools_by_name = {t.__name__: t for t in tools}\n",
    "    traj.tools = [convert_to_openai_tool(t) for t in tools]\n",
    "\n",
    "    client = AsyncOpenAI(\n",
    "        base_url=model.inference_base_url,\n",
    "        api_key=model.inference_api_key,\n",
    "    )\n",
    "\n",
    "    for _ in range(MAX_TURNS):\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model.get_inference_name(),\n",
    "            temperature=1,\n",
    "            messages=traj.messages(),\n",
    "            tools=traj.tools,\n",
    "        )\n",
    "\n",
    "        response_message = response.choices[0].message\n",
    "        traj.messages_and_choices.append(response.choices[0])\n",
    "\n",
    "        if not response_message.tool_calls:\n",
    "            return traj\n",
    "\n",
    "        try:\n",
    "            for tool_call in response_message.tool_calls:\n",
    "                tool_name: str = tool_call.function.name\n",
    "                if tool_name in tools_by_name:\n",
    "                    tool_args = json.loads(tool_call.function.arguments)\n",
    "                    tool_to_call = tools_by_name[tool_name]\n",
    "                    result = tool_to_call(**tool_args)\n",
    "                    traj.messages_and_choices.append(\n",
    "                        {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"name\": tool_name,\n",
    "                            \"content\": str(result),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    if tool_name == \"return_final_answer\":\n",
    "                        traj.final_answer = result\n",
    "                        # Score trajectory\n",
    "                        if traj.final_answer and scenario.gold_answer:\n",
    "                            judge_response = await judge_correctness(\n",
    "                                scenario, traj.final_answer.answer\n",
    "                            )\n",
    "                            traj.metrics[\"correct\"] = float(judge_response.accept)\n",
    "                        return traj\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return traj\n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "# LOCAL TRAINING - uses YOUR A100s\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",  # Automatically uses all 4 A100s\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ… Model loaded on YOUR A100s with LoRA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f5c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
